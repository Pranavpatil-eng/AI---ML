{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Classification Model Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F1-Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Evaluate a binary classifier for spam detection using accuracy, precision, recall and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Example true labels and predictions\n",
    "y_true = np.array([0, 1, 0, 1, 1, 0, 1, 0, 1, 1])  # 0: Not Spam, 1: Spam\n",
    "y_pred = np.array([0, 1, 0, 0, 1, 0, 1, 0, 1, 1])  # Predictions from a model\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"üìä Evaluation Metrics for Spam Detection:\")\n",
    "print(f\"‚úÖ Accuracy : {accuracy:.2f}\")\n",
    "print(f\"üéØ Precision: {precision:.2f}\")\n",
    "print(f\"üîÅ Recall   : {recall:.2f}\")\n",
    "print(f\"üìê F1 Score : {f1:.2f}\")\n",
    "\n",
    "# Optional: full classification report\n",
    "print(\"\\nüßæ Classification Report:\\n\", classification_report(y_true, y_pred, target_names=[\"Not Spam\", \"Spam\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Task 2: Compare performance of a multi-class classifier on recognizing animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Simulated true and predicted labels for a 3-class animal recognition task\n",
    "# Classes: 0 = Cat, 1 = Dog, 2 = Rabbit\n",
    "y_true = np.array([0, 1, 2, 0, 1, 2, 2, 0, 1, 2])\n",
    "y_pred = np.array([0, 1, 2, 1, 1, 2, 0, 0, 2, 2])\n",
    "\n",
    "# Class names\n",
    "class_names = [\"Cat\", \"Dog\", \"Rabbit\"]\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Results\n",
    "print(\"üìä Multi-Class Classifier Performance (Animal Recognition):\")\n",
    "print(f\"‚úÖ Accuracy       : {accuracy:.2f}\")\n",
    "print(f\"üéØ Precision (avg): {macro_precision:.2f}\")\n",
    "print(f\"üîÅ Recall (avg)   : {macro_recall:.2f}\")\n",
    "print(f\"üìê F1 Score (avg) : {macro_f1:.2f}\")\n",
    "\n",
    "# Detailed per-class report\n",
    "print(\"\\nüßæ Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Analyze classifier performance for predicting disease outbreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = np.array([0, 1, 0, 1, 1, 0, 1, 0, 1, 1])\n",
    "y_pred = np.array([0, 1, 0, 0, 1, 0, 1, 0, 1, 1])\n",
    "y_proba = np.array([0.2, 0.8, 0.1, 0.4, 0.85, 0.3, 0.9, 0.2, 0.75, 0.95])  \n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "\n",
    "print(\"üìä Disease Outbreak Classifier Performance:\")\n",
    "print(f\"‚úÖ Accuracy : {accuracy:.2f}\")\n",
    "print(f\"üéØ Precision: {precision:.2f}\")\n",
    "print(f\"üîÅ Recall   : {recall:.2f}\")\n",
    "print(f\"üìê F1 Score : {f1:.2f}\")\n",
    "print(f\"üìà AUC-ROC  : {roc_auc:.2f}\")\n",
    "print(\"\\nüßæ Classification Report:\\n\", classification_report(y_true, y_pred, target_names=[\"No Outbreak\", \"Outbreak\"]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nüîç Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve ‚Äì Disease Outbreak Prediction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
