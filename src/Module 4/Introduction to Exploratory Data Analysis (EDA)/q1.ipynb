{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Understanding the Dataset \n",
    "<br>\n",
    "Description: Load a dataset and understand its basic properties including data types dimensions, and first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a simple DataFrame\n",
    "data = {'col1': [1, 2, 3, 4, 5],\n",
    "        'col2': ['a', 'b', 'c', 'd', 'e'],\n",
    "        'col3': [1.1, 2.2, 3.3, 4.4, 5.5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Understand basic properties\n",
    "\n",
    "# Data types of each column\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Dimensions of the DataFrame (number of rows and columns)\n",
    "print(\"Dimensions:\")\n",
    "print(df.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Number of rows\n",
    "print(\"Number of Rows:\")\n",
    "print(len(df))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Number of columns\n",
    "print(\"Number of Columns:\")\n",
    "print(len(df.columns))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Index of the DataFrame (row labels)\n",
    "print(\"Index:\")\n",
    "print(df.index)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Columns of the DataFrame (column labels)\n",
    "print(\"Columns:\")\n",
    "print(df.columns)\n",
    "print(\"\\n\")\n",
    "\n",
    "# First few rows of the DataFrame\n",
    "print(\"First 5 Rows:\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# First n rows (e.g., first 3 rows)\n",
    "n = 3\n",
    "print(f\"First {n} Rows:\")\n",
    "print(df.head(n))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Concise summary of the DataFrame, including data types and non-null values\n",
    "print(\"DataFrame Info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Checking for Missing Values\n",
    "<br>\n",
    "Description: Identify missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with some missing values\n",
    "data = {'col1': [1, 2, np.nan, 4, 5],\n",
    "        'col2': ['a', np.nan, 'c', 'd', 'e'],\n",
    "        'col3': [1.1, 2.2, 3.3, np.nan, 5.5],\n",
    "        'col4': [True, False, True, True, np.nan]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify missing values\n",
    "\n",
    "# 1. Check for NaN (Not a Number) values in the entire DataFrame\n",
    "print(\"DataFrame with Missing Values:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Boolean DataFrame indicating missing values (True if missing, False otherwise):\")\n",
    "print(df.isnull())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Count the number of missing values in each column\n",
    "print(\"Number of missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Count the total number of missing values in the entire DataFrame\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f\"Total number of missing values in the DataFrame: {total_missing}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Get the percentage of missing values in each column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"Percentage of missing values per column:\")\n",
    "print(missing_percentage)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 5. Check if there are any missing values in the entire DataFrame (returns True if any missing value exists)\n",
    "has_missing = df.isnull().any().any()\n",
    "print(f\"Are there any missing values in the DataFrame? {has_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Descriptive Statistics\n",
    "<br>\n",
    "Description: Calculate descriptive statistics for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with numerical and non-numerical columns\n",
    "data = {'col1': [1, 2, 3, 4, 5],\n",
    "        'col2': ['a', 'b', 'c', 'd', 'e'],\n",
    "        'col3': [1.1, 2.2, 3.3, 4.4, 5.5],\n",
    "        'col4': [10, 20, 30, 40, 50],\n",
    "        'col5': [np.nan, 2, 3, np.nan, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate descriptive statistics for numerical columns\n",
    "\n",
    "# 1. Get descriptive statistics for all numerical columns\n",
    "print(\"Descriptive Statistics for all numerical columns:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Get descriptive statistics for a specific numerical column\n",
    "column_name = 'col3'\n",
    "print(f\"Descriptive Statistics for column '{column_name}':\")\n",
    "print(df[column_name].describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Calculate specific descriptive statistics for numerical columns\n",
    "\n",
    "# Mean of all numerical columns\n",
    "print(\"Mean of numerical columns:\")\n",
    "print(df.mean(numeric_only=True))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Mean of a specific numerical column\n",
    "print(f\"Mean of column '{column_name}': {df[column_name].mean()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Median of all numerical columns\n",
    "print(\"Median of numerical columns:\")\n",
    "print(df.median(numeric_only=True))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Standard deviation of all numerical columns\n",
    "print(\"Standard deviation of numerical columns:\")\n",
    "print(df.std(numeric_only=True))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Minimum value of all numerical columns\n",
    "print(\"Minimum value of numerical columns:\")\n",
    "print(df.min(numeric_only=True))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Maximum value of all numerical columns\n",
    "print(\"Maximum value of numerical columns:\")\n",
    "print(df.max(numeric_only=True))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Count of non-missing values in each column\n",
    "print(\"Count of non-missing values in each column:\")\n",
    "print(df.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Quantiles of all numerical columns (e.g., 25th, 50th, 75th percentiles)\n",
    "print(\"Quantiles of numerical columns:\")\n",
    "print(df.quantile([0.25, 0.5, 0.75], numeric_only=True))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Handling Outliers\n",
    "<br>\n",
    "Description: Identify outliers in numerical columns using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with some potential outliers\n",
    "data = {'col1': [1, 2, 3, 4, 5, 100],\n",
    "        'col2': [10, 12, 15, 13, 11, 200],\n",
    "        'col3': [1.1, 2.2, 3.3, 4.4, 5.5, -10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify outliers using box plots\n",
    "\n",
    "# 1. Box plots for all numerical columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df)\n",
    "plt.title('Box Plots for All Numerical Columns')\n",
    "plt.ylabel('Values')\n",
    "plt.show()\n",
    "\n",
    "# 2. Box plot for a specific numerical column\n",
    "column_name = 'col1'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df[column_name])\n",
    "plt.title(f'Box Plot for Column: {column_name}')\n",
    "plt.xlabel('Values')\n",
    "plt.show()\n",
    "\n",
    "# 3. Multiple box plots in subplots for better visualization\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols = len(numerical_cols)\n",
    "fig, axes = plt.subplots(1, num_cols, figsize=(15, 5))\n",
    "fig.suptitle('Box Plots for Numerical Columns', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.boxplot(x=df[col], ax=axes[i])\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Values')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# Explanation of Box Plots for Outlier Detection:\n",
    "\n",
    "# A box plot visually represents the distribution of a dataset based on five summary statistics:\n",
    "# - Minimum: The smallest value in the dataset (excluding outliers).\n",
    "# - First Quartile (Q1): The 25th percentile; 25% of the data falls below this value.\n",
    "# - Median (Q2): The 50th percentile; the middle value of the dataset.\n",
    "# - Third Quartile (Q3): The 75th percentile; 75% of the data falls below this value.\n",
    "# - Maximum: The largest value in the dataset (excluding outliers).\n",
    "\n",
    "# The box itself spans from Q1 to Q3, and the line inside the box represents the median.\n",
    "# The \"whiskers\" extend from the box to show the range of the data. By default, they typically extend to 1.5 times the Interquartile Range (IQR = Q3 - Q1) from the edges of the box.\n",
    "\n",
    "# Outliers are data points that fall outside the whiskers. These are often plotted as individual points.\n",
    "\n",
    "# In the generated box plots:\n",
    "# - Points that appear far above the upper whisker or far below the lower whisker are potential outliers.\n",
    "# - Visual inspection of these plots helps in identifying columns that might contain extreme values.\n",
    "\n",
    "# Further Steps for Handling Outliers (Beyond Identification):\n",
    "# Once outliers are identified, you might need to decide how to handle them. Common approaches include:\n",
    "# - Removing the outliers from the dataset.\n",
    "# - Transforming the data (e.g., using logarithms or scaling) to reduce the impact of outliers.\n",
    "# - Imputing outlier values with more reasonable values (e.g., using the median or a capped value).\n",
    "# - Keeping the outliers if they represent genuine and important variations in the data.\n",
    "\n",
    "# Note: The definition of an outlier can sometimes be context-dependent, and visual inspection using box plots is a good starting point for investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Question 5: Categorical Data Analysis\n",
    "<br>\n",
    "Description: Explore the counts of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with categorical columns\n",
    "data = {'category1': ['A', 'B', 'A', 'C', 'B', 'A', 'A'],\n",
    "        'category2': ['X', 'Y', 'X', 'Z', 'Y', 'Y', 'X'],\n",
    "        'value': [10, 20, 15, 25, 30, 12, 18]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Explore the counts of categorical variables\n",
    "\n",
    "# 1. Get value counts for a single categorical column\n",
    "column_name = 'category1'\n",
    "print(f\"Value counts for column '{column_name}':\")\n",
    "print(df[column_name].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get value counts with percentages\n",
    "print(f\"Value counts (with percentages) for column '{column_name}':\")\n",
    "print(df[column_name].value_counts(normalize=True) * 100)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Get value counts for another categorical column\n",
    "column_name = 'category2'\n",
    "print(f\"Value counts for column '{column_name}':\")\n",
    "print(df[column_name].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Get value counts for all categorical columns (iterating through object/string columns)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "if not categorical_cols.empty:\n",
    "    print(\"Value counts for all categorical columns:\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nValue counts for column '{col}':\")\n",
    "        print(df[col].value_counts())\n",
    "else:\n",
    "    print(\"No categorical columns (of object dtype) found in the DataFrame.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Note: Categorical columns might also be of 'category' dtype.\n",
    "# Let's include that in our selection as well.\n",
    "categorical_cols_all = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "if not categorical_cols_all.empty:\n",
    "    print(\"Value counts for all categorical columns (including 'category' dtype):\")\n",
    "    for col in categorical_cols_all:\n",
    "        print(f\"\\nValue counts for column '{col}':\")\n",
    "        print(df[col].value_counts())\n",
    "else:\n",
    "    print(\"No categorical columns (of object or category dtype) found in the DataFrame.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Example with a 'category' dtype column\n",
    "data_with_category = {'category_type': pd.Series(['Type1', 'Type2', 'Type1', 'Type3', 'Type2'], dtype='category'),\n",
    "                       'value': [1, 2, 3, 4, 5]}\n",
    "df_category = pd.DataFrame(data_with_category)\n",
    "\n",
    "print(\"DataFrame with 'category' dtype:\")\n",
    "print(df_category)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Value counts for 'category_type' (category dtype):\")\n",
    "print(df_category['category_type'].value_counts())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Data Transformation\n",
    "<br>\n",
    "Description: Transform a categorical column into numerical using Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's use the DataFrame from the previous question\n",
    "data = {'category': ['A', 'B', 'A', 'C', 'B', 'A', 'A'],\n",
    "        'value': [10, 20, 15, 25, 30, 12, 18]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Select the categorical column to transform\n",
    "categorical_column = 'category'\n",
    "\n",
    "# Fit and transform the categorical column\n",
    "df['category_encoded'] = label_encoder.fit_transform(df[categorical_column])\n",
    "\n",
    "print(\"DataFrame after Label Encoding:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# To understand the mapping (optional)\n",
    "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Encoding Mapping:\")\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Visualizing Data Distributions\n",
    "<br>\n",
    "Description: Plot histograms for numerical columns to understand distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with numerical columns\n",
    "data = {'col1': [1, 2, 2, 3, 3, 3, 4, 4, 5],\n",
    "        'col2': [10, 12, 11, 13, 12, 14, 15, 14, 13],\n",
    "        'col3': [1.1, 2.2, 2.5, 3.1, 3.5, 3.8, 4.2, 4.5, 4.8],\n",
    "        'col4': [100, 150, 120, 180, 200, 130, 160, 190, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Visualize data distributions using histograms\n",
    "\n",
    "# 1. Histogram for a single numerical column\n",
    "column_name = 'col1'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df[column_name], kde=True) # kde=True adds a kernel density estimate line\n",
    "plt.title(f'Distribution of {column_name}')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 2. Histograms for all numerical columns in subplots\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols = len(numerical_cols)\n",
    "fig, axes = plt.subplots(1, num_cols, figsize=(15, 5))\n",
    "fig.suptitle('Distributions of Numerical Columns', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.histplot(df[col], ax=axes[i], kde=True)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Values')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# Explanation of Histograms:\n",
    "\n",
    "# A histogram is a graphical representation of the distribution of a dataset.\n",
    "# It groups the data into \"bins\" (intervals) and displays the frequency (or count)\n",
    "# of data points that fall into each bin as bars.\n",
    "\n",
    "# Key aspects of a histogram:\n",
    "# - Bins: The intervals on the x-axis that divide the range of the data.\n",
    "# - Frequency: The height of each bar, representing the number of data points within that bin.\n",
    "# - Shape: The overall shape of the histogram can provide insights into the distribution of the data (e.g., normal, skewed, uniform, bimodal).\n",
    "\n",
    "# Using `sns.histplot()`:\n",
    "# - The `sns.histplot()` function from seaborn provides a convenient way to create histograms.\n",
    "# - The first argument is the data (a pandas Series representing the column).\n",
    "# - `kde=True` adds a Kernel Density Estimate (KDE) line to the plot. The KDE provides a smoothed estimate of the probability density function of the variable.\n",
    "\n",
    "# Interpreting Histograms:\n",
    "# - Symmetry: A symmetric histogram suggests that the data is evenly distributed around the center.\n",
    "# - Skewness:\n",
    "#   - Right-skewed (positive skew): The tail on the right side is longer, and most of the data is concentrated on the left.\n",
    "#   - Left-skewed (negative skew): The tail on the left side is longer, and most of the data is concentrated on the right.\n",
    "# - Modality:\n",
    "#   - Unimodal: One peak in the distribution.\n",
    "#   - Bimodal: Two peaks in the distribution, which might suggest the presence of two distinct groups within the data.\n",
    "#   - Multimodal: More than two peaks.\n",
    "# - Spread: The width of the distribution indicates the variability of the data.\n",
    "# - Outliers: Extreme values that are far from the main body of the distribution might appear as isolated bars at the edges.\n",
    "\n",
    "# Histograms are a fundamental tool for Exploratory Data Analysis (EDA) as they help in understanding the underlying patterns and characteristics of your numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Correlation Analysis\n",
    "<br>\n",
    "Description: Calculate and visualize the correlation matrix for numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with numerical features\n",
    "data = {'feature1': [1, 2, 3, 4, 5, 6],\n",
    "        'feature2': [2, 4, 5, 4, 6, 7],\n",
    "        'feature3': [5, 4, 6, 7, 8, 9],\n",
    "        'feature4': [10, 12, 15, 13, 16, 18]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a categorical column to show it's excluded from correlation\n",
    "df['category'] = ['A', 'B', 'A', 'C', 'B', 'C']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.select_dtypes(include=np.number).corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# Explanation of Correlation Analysis:\n",
    "\n",
    "# Correlation measures the linear relationship between two numerical variables.\n",
    "# The correlation coefficient ranges from -1 to +1:\n",
    "# - +1 indicates a perfect positive linear relationship (as one variable increases, the other also increases proportionally).\n",
    "# - -1 indicates a perfect negative linear relationship (as one variable increases, the other decreases proportionally).\n",
    "# - 0 indicates no linear relationship between the two variables.\n",
    "\n",
    "# The Pearson correlation coefficient is the most common method used.\n",
    "\n",
    "# Steps:\n",
    "# 1. Select Numerical Features: We first select only the columns with numerical data types, as correlation analysis is typically applied to these.\n",
    "# 2. Calculate the Correlation Matrix: The `.corr()` method in pandas calculates the pairwise correlation between all numerical columns in the DataFrame. The result is a correlation matrix, where each cell (i, j) represents the correlation between the i-th and j-th columns.\n",
    "# 3. Visualize the Correlation Matrix (Heatmap): A heatmap is a graphical representation of the correlation matrix, where the color intensity and hue represent the strength and direction of the correlation.\n",
    "#    - `sns.heatmap()` from seaborn is used for this visualization.\n",
    "#    - `correlation_matrix` is the data to be plotted.\n",
    "#    - `annot=True` displays the correlation values on the heatmap.\n",
    "#    - `cmap='coolwarm'` sets the color scheme (you can explore other colormaps). 'coolwarm' typically uses red for positive correlations and blue for negative correlations, with white or light colors near zero.\n",
    "#    - `fmt=\".2f\"` formats the correlation values to two decimal places.\n",
    "#    - `linewidths=.5` adds lines between the cells for better separation.\n",
    "#    - `plt.title()` sets the title of the plot.\n",
    "#    - `plt.show()` displays the heatmap.\n",
    "\n",
    "# Interpreting the Heatmap:\n",
    "# - Look for cells with strong colors (dark red or dark blue), as these indicate high positive or negative correlations, respectively.\n",
    "# - Values close to 0 (lighter colors) indicate weak or no linear correlation.\n",
    "# - The diagonal of the matrix will always have a correlation of 1 (a variable is perfectly correlated with itself).\n",
    "# - The matrix is symmetric (the correlation between A and B is the same as between B and A).\n",
    "\n",
    "# Importance of Correlation Analysis:\n",
    "# - Feature Selection: Identifying highly correlated features can help in reducing redundancy in your dataset. You might choose to keep only one of the highly correlated features.\n",
    "# - Understanding Relationships: Correlation analysis helps in understanding how different numerical variables relate to each other, which can provide insights into the underlying processes in your data.\n",
    "# - Detecting Multicollinearity: High correlation between independent variables in a regression model can lead to multicollinearity issues.\n",
    "\n",
    "# Note: Correlation only measures linear relationships. Two variables can have a strong non-linear relationship but a weak linear correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Feature Engineering\n",
    "<br>\n",
    "Description: Create a new feature by combining or transforming existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with some numerical features\n",
    "data = {'price': [100, 150, 120, 180, 200],\n",
    "        'quantity': [5, 2, 10, 3, 4],\n",
    "        'discount': [0.1, 0.05, 0, 0.15, 0.2]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 1. Creating a new feature by combining existing numerical features (e.g., total revenue)\n",
    "df['total_revenue'] = df['price'] * df['quantity']\n",
    "print(\"DataFrame with 'total_revenue' feature:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Creating a new feature by transforming an existing numerical feature (e.g., log transformation)\n",
    "import numpy as np\n",
    "# Adding a small constant to avoid log(0) if there are zero values\n",
    "df['price_log'] = np.log(df['price'])\n",
    "print(\"DataFrame with 'price_log' feature:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Creating a new categorical feature based on existing numerical feature (e.g., price range)\n",
    "def categorize_price(price):\n",
    "    if price < 120:\n",
    "        return 'Low'\n",
    "    elif price < 180:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['price_range'] = df['price'].apply(categorize_price)\n",
    "print(\"DataFrame with 'price_range' feature:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Creating a new binary feature based on an existing numerical feature (e.g., discount applied)\n",
    "df['discount_applied'] = df['discount'].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(\"DataFrame with 'discount_applied' feature:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 5. Creating a new feature by combining a numerical and a categorical feature (not directly, but can be part of a more complex transformation)\n",
    "# For example, if we had regions and sales, we might calculate average sales per region.\n",
    "# This often involves grouping and aggregation, which is a separate topic but related to feature engineering.\n",
    "\n",
    "# Explanation of Feature Engineering:\n",
    "\n",
    "# Feature engineering is the process of creating new features from existing data\n",
    "# that may be more informative or better suited for machine learning algorithms.\n",
    "# It often involves domain knowledge and creativity.\n",
    "\n",
    "# Common Feature Engineering Techniques Demonstrated Above:\n",
    "\n",
    "# 1. Combining Numerical Features:\n",
    "#    - We created 'total_revenue' by multiplying 'price' and 'quantity'. This new feature\n",
    "#      can be more relevant for understanding the financial aspect of the data.\n",
    "\n",
    "# 2. Transforming Numerical Features:\n",
    "#    - We applied a logarithmic transformation to the 'price' column to create 'price_log'.\n",
    "#    - Log transformation can help in:\n",
    "#      - Reducing skewness in the data.\n",
    "#      - Making the distribution more normal.\n",
    "#      - Stabilizing variance.\n",
    "#      - Handling features with different scales.\n",
    "\n",
    "# 3. Creating Categorical Features from Numerical Features (Binning or Discretization):\n",
    "#    - We created 'price_range' by categorizing the 'price' into 'Low', 'Medium', and 'High' based on predefined thresholds.\n",
    "#    - This can be useful for algorithms that work better with categorical inputs or for simplifying complex numerical relationships.\n",
    "\n",
    "# 4. Creating Binary Features:\n",
    "#    - We created 'discount_applied' which is 1 if a discount was given (discount > 0) and 0 otherwise.\n",
    "#    - Binary features can be useful for indicating the presence or absence of a certain condition.\n",
    "\n",
    "# 5. Combining Numerical and Categorical Features (Indirectly):\n",
    "#    - While not a direct element-wise operation like the others, combining different types of features often involves grouping data by a categorical feature and then calculating statistics (e.g., mean, sum) of a numerical feature within each group. This creates new numerical features that are informed by the categories.\n",
    "\n",
    "# The choice of feature engineering techniques depends heavily on the specific dataset, the problem being solved, and the machine learning models being used. Effective feature engineering can significantly improve the performance of predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Advanced Outlier Detection\n",
    "<br>\n",
    "Description: Use the Z-score method to identify and handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load a dataset (replace 'your_dataset.csv' with the actual path to your file)\n",
    "# For demonstration, let's create a DataFrame with a numerical column and some outliers\n",
    "data = {'values': [10, 12, 15, 13, 11, 100, 120, 14, 16, -50]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Define a function to identify outliers using the Z-score method\n",
    "def identify_outliers_zscore(series, threshold=3):\n",
    "    \"\"\"\n",
    "    Identifies outliers in a pandas Series using the Z-score method.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The numerical Series to check for outliers.\n",
    "        threshold (float): The Z-score threshold. Data points with an absolute\n",
    "                           Z-score greater than this threshold are considered outliers.\n",
    "                           Commonly used values are 2 or 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A boolean Series indicating whether each data point is an outlier (True) or not (False).\n",
    "    \"\"\"\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    z_scores = np.abs((series - mean) / std)\n",
    "    return z_scores > threshold\n",
    "\n",
    "# Identify outliers in the 'values' column with a threshold of 3\n",
    "outliers_mask = identify_outliers_zscore(df['values'], threshold=3)\n",
    "print(\"Outliers (Boolean Mask):\")\n",
    "print(outliers_mask)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the actual outlier values\n",
    "outliers = df[outliers_mask]\n",
    "print(\"Outlier Values:\")\n",
    "print(outliers)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Handle outliers (example: replace with NaN)\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned.loc[outliers_mask, 'values'] = np.nan\n",
    "print(\"DataFrame after replacing outliers with NaN:\")\n",
    "print(df_cleaned)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Handle outliers (example: capping/flooring)\n",
    "def cap_floor_outliers_zscore(series, threshold=3):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    z_scores = np.abs((series - mean) / std)\n",
    "    upper_bound = mean + threshold * std\n",
    "    lower_bound = mean - threshold * std\n",
    "    capped_series = series.copy()\n",
    "    capped_series[series > upper_bound] = upper_bound\n",
    "    capped_series[series < lower_bound] = lower_bound\n",
    "    return capped_series\n",
    "\n",
    "df_capped = df.copy()\n",
    "df_capped['values_capped'] = cap_floor_outliers_zscore(df['values'], threshold=3)\n",
    "print(\"DataFrame after capping/flooring outliers:\")\n",
    "print(df_capped)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explanation of the Z-score Method for Outlier Detection:\n",
    "\n",
    "# The Z-score (or standard score) measures how many standard deviations a data point\n",
    "# is away from the mean of the dataset.\n",
    "\n",
    "# Steps:\n",
    "# 1. Calculate the Mean (μ) of the numerical column.\n",
    "# 2. Calculate the Standard Deviation (σ) of the numerical column.\n",
    "# 3. For each data point (x) in the column, calculate the Z-score using the formula:\n",
    "#    Z = (x - μ) / σ\n",
    "\n",
    "# Identifying Outliers:\n",
    "# - A common rule of thumb is to consider data points with an absolute Z-score greater\n",
    "#   than a certain threshold as outliers.\n",
    "# - Typical thresholds are 2 or 3. A threshold of 3 means that data points more than 3\n",
    "#   standard deviations away from the mean are flagged as outliers.\n",
    "\n",
    "# Handling Outliers using Z-score:\n",
    "# Once outliers are identified using the Z-score, several strategies can be employed\n",
    "# to handle them:\n",
    "\n",
    "# 1. Removal: The simplest approach is to remove the rows containing the outliers.\n",
    "#    However, this can lead to loss of information, especially if outliers are frequent\n",
    "#    or if they represent important extreme values.\n",
    "\n",
    "# 2. Replacement: Outliers can be replaced with other values:\n",
    "#    - Mean or Median Imputation: Replace outliers with the mean or median of the\n",
    "#      non-outlier data. This can reduce the impact of extreme values but might\n",
    "#      affect the distribution.\n",
    "#    - Capping and Flooring: Replace outliers above a certain upper bound with the\n",
    "#      upper bound, and outliers below a lower bound with the lower bound. The bounds\n",
    "#      can be determined based on the Z-score and standard deviation.\n",
    "\n",
    "# 3. Transformation: Applying transformations like logarithmic or winsorizing can\n",
    "#    reduce the impact of outliers without removing them. Winsorizing involves\n",
    "#    replacing extreme values with values at a specific percentile (e.g., 5th and 95th).\n",
    "\n",
    "# In the code above, we demonstrated:\n",
    "# - Identifying outliers using a Z-score threshold of 3.\n",
    "# - Replacing the identified outliers with NaN.\n",
    "# - Capping and flooring the outliers based on the Z-score and standard deviation.\n",
    "\n",
    "# Choosing the appropriate method for handling outliers depends on the nature of\n",
    "# the data, the reasons for the outliers, and the specific goals of the analysis\n",
    "# or modeling task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
